1. privileged things (dynarray)
	1.1 higher judgment functions. the higher judgment function is a quine, and occasionally makes copies of itself. contains many copies of the same command, so that randomness doesn't suddenly run out (we don't want our user to become a Turing machine)
		on new-user event, it runs on the directed judgment function.
		on new-country event, it runs on itself.
	1.2 random judgment. works once per encryption run. copies and mutates judgment functions. 
	1.3 directed judgment functions. works like the immune system, many judgments per encryption run. tries to optimize by splitting and duplicating good efforts.
	how do we store information on each of these? every time we copy it, we ought to copy some comments...
		we store 2. if something is going horribly wrong, we revert it.
having few tiers of judgment is good, because the more tiers there are, the harder it is to improve them.
	directed judgement must operate, so each encryption run must consist of several times.
	random judgment must run, so there must be several encryption runs.

problem: everything must have a responsible owner, who is penalized if it goes wrong. for example, if we have language, then the language info must be stored across time.
	however, if immediate judgment fucks it up, it won't really suffer any consequences, because language info isn't tested. same for long-lasting objects like cooperative locks, and circular buffers. judgment can screw it up for speed boosts, and then nobody suffers any consequences.

1. big set of things (dynarray)
	1.1 working functions that directly feed back into judgment. mostly uses 1.3, the goodness indicators
	1.2 working functions. touches everything.
	1.3 goodness indicators (mostly integers)
	1.4, 1.5... mostly integers. sometimes commented integers.
to store comments on things, we use dynamic pointers. the dynamic pointer to the big array stores more dynarrays, which contain the object of interest in the first slot. we don't point to the object directly.

1. we have a directed-judgment function. it randomly gets functions from the big tree of things and then modifies them. it should have some purpose. 
to do this, it picks a function, looks for static-object ASTs, and changes them. but it remembers the old values.
	then it runs the simulation, assigns goodness, and decides to revert or not.
	it creates random functions, and inserts calls to them (fuzztester)
	
but then, it also looks for random functions, copies AST. then it snips out part of them.
	then it goes to another function, and then tries inserting the new segment inside. maybe making a few tweaks.
	there is a huge mess of global integers that are randomly modified. basically, neurotransmitter levels. there are some functions which store constant values into them, some functions which grab some levels and then randomly store into other levels.
		we will rely on this for 
	loops should be able to modify their condition somehow, and are under great suspicion.
how can it find ASTs? that's because all the ASTs must be stored in a vector.
	they should all be versioned, as well. with some level of "goodness". when you fail to make a change, the "stability" value goes up. high stability = low rate of changing.
2. we have a goodness function. somehow, it produces an evaluation of how well you did, and encodes them in some global variables somewhere.
	then, the directed-judgment function uses these special goodness values to do something.
3. random-judgment function.
	randomly changes the goodness function and the directed-judgment function, and itself. also gets values from goodness, to randomly decide its behavior.
	it also makes copies of the directed-judgment function, occasionally kills them.
	these functions ought to be run in parallel.
4. culling function. we don't want memory to explode. so this takes especially useless functions and throws them away.
how do we decide which functions should be called?
	there should be some "reset" functions, that happen when something really bad happens. after a long string of badness, these functions are called.
problem: we can't take in negative feedback, only positive feedback. this is because our changes don't have true negatives, since they're random and arbitrary.

things that are important are repeated many times. just like in DNA.

Function recombination: modification of existing functions. each step is repeated as many times as necessary.
	after each change, we should generally test if the function will still compile. if it won't compile, we must roll-back. roll-back is necessarily baked into the function.
1. browsing mode
	1. choose a random function to obtain from
	2. snip the function by choosing a head, then copying
2. insertion mode
	1. in some other function, find some location by randomly browsing.
	2. find a snipping location by going down previous_BB pointers, and choosing two points.
		insert the head in the top of the snip.
		browse through the snippet's backpointers to find the bottom. change the bottom's AST to point to the original snip.
3. random modification. insertion adds in code, but now we have to mix the code to make it interact.
	1. randomly browse the function, memorizing AST locations.
	2. randomly change some AST locations to other ASTs.
	3. randomly load imv values. do some modification to them, by incrementing/decrementing them randomly (this should be passed off to another function)

note: when splicing DNA, we can splice from a function to itself.
	and some level of locality is good too.
	for example, with the AST tags, you want to have lots of casework. in that case, ideal splicing involves making lots of if statements, one for each AST tag.
	thus, we splice from a function to itself, and also from functions to nearby functions.

judgment must constantly take in information from the rest of the user.
	for example, when choosing an AST to make, it better outsource this capability.

for ideas that we want to happen, we need to provide meaningful initial implementations. so: values get the old "first value is goodness, second value is information" trick. and then the user regresses on these.
	so the anti-encryption breaker should provide a meaningful correlation, to correlate fields with other fields.
	try to find patterns. if you do find a pattern, immediately duplicate yourself.
		should the duplication be external or internal? external. because otherwise, when modifying, there's potential for cancer.
	and we need to verify that the pattern actually is correctly found. so an external judge will score various different contestants.
basically, here we're testing the judgment function for a single encryption test run (consisting of a single function, and a series of output numbers).
	each judgment function owns a bunch of things.
	and there are many judgment functions for the single encryption test run.

how to stop cancer? probably by limiting the various sources of new things, randomly permuting them, and culling them whenever necessary.

Model:
	1. judgment runs a "testme" AST. this sets up the encryptor, which runs the default runner a lot.
	2. judgment gets to run again after the test suite is done. it compares the time it took to finish the suite. if it's really excessive, then revert.
	3. if performance is worse, then revert.
	4. problem: how do we actually revert? I think we have to snapshot the entire stack.